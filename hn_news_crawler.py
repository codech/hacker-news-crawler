#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Hacker News çˆ¬è™« - å®Œæ•´ç‰ˆæœ¬

åŠŸèƒ½ç‰¹æ€§ï¼š
- è·å–HNé¦–é¡µæ‰€æœ‰æ–°é—»ï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰
- è‡ªåŠ¨ç¿»è¯‘æ ‡é¢˜å’Œå†…å®¹æ‘˜è¦
- æŒ‰æ—¥æœŸå­˜å‚¨åˆ°CSVæ–‡ä»¶
- å‘é€æ‰€æœ‰æœªå‘é€æ–°é—»åˆ°Telegram
- æ”¯æŒä»£ç†é…ç½®
- è‡ªåŠ¨å»é‡å’Œé”™è¯¯å¤„ç†

ä½œè€…: Bruce Chen
ç‰ˆæœ¬: 2.0
æ›´æ–°: 2025-05-24
"""

import os
import sys
import json
import time
import asyncio
import logging
import schedule
import threading
import subprocess
import urllib.parse
import csv
import pandas as pd
from datetime import datetime, timedelta
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from telegram import Bot
import httpx

class HackerNewsCrawler:
    def __init__(self):
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv('config.env')
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–æ‰€æœ‰é…ç½®
        self.base_url = os.getenv('BASE_URL', 'https://news.ycombinator.com')
        self.max_news_count = int(os.getenv('MAX_NEWS_COUNT', 100))
        self.min_score = int(os.getenv('MIN_SCORE', 0))
        
        # ç½‘ç»œé…ç½®
        self.request_timeout = int(os.getenv('REQUEST_TIMEOUT', 15))
        self.connection_test_timeout = int(os.getenv('CONNECTION_TEST_TIMEOUT', 10))
        self.translation_timeout = int(os.getenv('TRANSLATION_TIMEOUT', 10))
        self.telegram_timeout = int(os.getenv('TELEGRAM_TIMEOUT', 15))
        self.max_retries = int(os.getenv('MAX_RETRIES', 3))
        self.request_interval = float(os.getenv('REQUEST_INTERVAL', 0.3))
        
        # æ¶ˆæ¯å‘é€é…ç½®
        self.message_send_interval = float(os.getenv('MESSAGE_SEND_INTERVAL', 1.0))
        self.message_retry_interval = float(os.getenv('MESSAGE_RETRY_INTERVAL', 2.0))
        self.bulk_message_interval = float(os.getenv('BULK_MESSAGE_INTERVAL', 3.0))
        self.message_max_retries = int(os.getenv('MESSAGE_MAX_RETRIES', 2))
        
        # åŠŸèƒ½å¼€å…³
        self.enable_translation = os.getenv('ENABLE_TRANSLATION', 'true').lower() == 'true'
        self.enable_content_summary = os.getenv('ENABLE_CONTENT_SUMMARY', 'true').lower() == 'true'
        
        # æ€§èƒ½é…ç½®
        self.max_translation_length = int(os.getenv('MAX_TRANSLATION_LENGTH', 400))
        self.max_summary_length = int(os.getenv('MAX_SUMMARY_LENGTH', 200))
        self.max_title_length = int(os.getenv('MAX_TITLE_LENGTH', 200))
        
        # Telegramé…ç½®
        self.bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
        self.chat_id = os.getenv('TELEGRAM_CHAT_ID')
        
        if not self.bot_token or not self.chat_id:
            raise ValueError("è¯·é…ç½®TELEGRAM_BOT_TOKENå’ŒTELEGRAM_CHAT_ID")
        
        # é…ç½®ä»£ç† - æ”¯æŒå¼€å…³æ§åˆ¶
        self.proxies = {}
        enable_proxy = os.getenv('ENABLE_PROXY', 'false').lower() == 'true'
        
        if enable_proxy:
            proxy_http = os.getenv('PROXY_HTTP') or os.getenv('http_proxy')
            proxy_https = os.getenv('PROXY_HTTPS') or os.getenv('https_proxy')
            
            if proxy_http:
                self.proxies['http'] = proxy_http
                self.proxies['https'] = proxy_https or proxy_http
                logging.info(f"âœ… ä»£ç†å·²å¯ç”¨: {proxy_http}")
            else:
                logging.warning("âš ï¸ ä»£ç†å¼€å…³å·²å¼€å¯ä½†æœªé…ç½®ä»£ç†åœ°å€ï¼Œä½¿ç”¨ç›´è¿æ¨¡å¼")
        else:
            logging.info("ğŸŒ ä»£ç†å¼€å…³å·²å…³é—­ï¼Œä½¿ç”¨ç›´è¿æ¨¡å¼")
        
        # åˆå§‹åŒ–Telegram Bot
        self.bot = Bot(token=self.bot_token)
        logging.info("ğŸ¤– Telegram Bot åˆå§‹åŒ–æˆåŠŸ")
        
        # HTTPè¯·æ±‚å¤´
        self.headers = {
            'User-Agent': os.getenv('USER_AGENT', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        }
        
        # CSVæ–‡ä»¶é…ç½®
        self.data_dir = os.getenv('DATA_DIR', 'data')
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)
        
        # ä»Šæ—¥CSVæ–‡ä»¶
        today = datetime.now().strftime('%Y-%m-%d')
        self.csv_file = os.path.join(self.data_dir, f'hn_news_{today}.csv')
        
        # CSVåˆ—å - ä»é…ç½®æ–‡ä»¶è¯»å–
        csv_columns_str = os.getenv('CSV_COLUMNS', 'id,title,title_cn,url,hn_url,score,comments,content_summary,content_summary_cn,crawl_time,sent_time,is_sent')
        self.csv_columns = [col.strip() for col in csv_columns_str.split(',')]
        
        # åˆå§‹åŒ–CSVæ–‡ä»¶
        self.init_csv_file()
        
        # é…ç½®æ—¥å¿—
        self.setup_logging()
        
        logging.info(f"CSVæ–‡ä»¶: {self.csv_file}")
    
    def setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_level = getattr(logging, os.getenv('LOG_LEVEL', 'INFO').upper())
        log_file = os.getenv('LOG_FILE', 'hn_crawler.log')
        log_format = os.getenv('LOG_FORMAT', '%(asctime)s - %(levelname)s - %(message)s')
        
        # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
        for handler in logging.root.handlers[:]:
            logging.root.removeHandler(handler)
        
        # é‡æ–°é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=log_level,
            format=log_format,
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
    
    def init_csv_file(self):
        """åˆå§‹åŒ–CSVæ–‡ä»¶"""
        if not os.path.exists(self.csv_file):
            with open(self.csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=self.csv_columns)
                writer.writeheader()
            logging.info(f"åˆ›å»ºæ–°çš„CSVæ–‡ä»¶: {self.csv_file}")
        else:
            # æ¸…ç†é‡å¤æ•°æ®
            self.clean_duplicate_data()
    
    def clean_duplicate_data(self):
        """æ¸…ç†CSVä¸­çš„é‡å¤æ•°æ®ï¼Œä¿ç•™æœ€æ–°çš„è®°å½•"""
        try:
            df = self.load_news_data()
            
            if df.empty:
                return
            
            original_count = len(df)
            
            # æŒ‰idå»é‡ï¼Œä¿ç•™æœ€åä¸€æ¡è®°å½•ï¼ˆæœ€æ–°çš„ï¼‰
            # å¯¹äºå·²å‘é€çš„è®°å½•ï¼Œä¼˜å…ˆä¿ç•™å·²å‘é€çŠ¶æ€
            df['is_sent'] = df['is_sent'].fillna(False).astype(bool)
            df = df.sort_values(['id', 'is_sent'], ascending=[True, False])
            df_cleaned = df.drop_duplicates(subset=['id'], keep='first')
            
            if len(df_cleaned) < original_count:
                # ä¿å­˜æ¸…ç†åçš„æ•°æ®
                df_cleaned.to_csv(self.csv_file, index=False, encoding='utf-8')
                removed_count = original_count - len(df_cleaned)
                logging.info(f"æ¸…ç†é‡å¤æ•°æ®: ç§»é™¤ {removed_count} æ¡é‡å¤è®°å½•ï¼Œä¿ç•™ {len(df_cleaned)} æ¡")
            else:
                logging.debug("æ²¡æœ‰å‘ç°é‡å¤æ•°æ®")
                
        except Exception as e:
            logging.error(f"æ¸…ç†é‡å¤æ•°æ®å¤±è´¥: {e}")
    
    def load_news_data(self):
        """åŠ è½½ä»Šæ—¥æ–°é—»æ•°æ®"""
        try:
            if os.path.exists(self.csv_file):
                df = pd.read_csv(self.csv_file)
                return df
            else:
                return pd.DataFrame(columns=self.csv_columns)
        except Exception as e:
            logging.error(f"åŠ è½½CSVæ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame(columns=self.csv_columns)
    
    def save_news_to_csv(self, news_item):
        """ä¿å­˜æ–°é—»åˆ°CSVï¼Œä¸¥æ ¼å»é‡"""
        try:
            df = self.load_news_data()
            
            # ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜ï¼šç»Ÿä¸€è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒ
            news_id = str(news_item['id'])
            existing = df[df['id'].astype(str) == news_id]
            
            if not existing.empty:
                # å¦‚æœè®°å½•å·²å­˜åœ¨ï¼Œåªæ›´æ–°åˆ†æ•°å’Œè¯„è®ºæ•°ï¼ˆè¿™äº›å¯èƒ½ä¼šå˜åŒ–ï¼‰
                mask = df['id'].astype(str) == news_id
                df.loc[mask, ['score', 'comments']] = [
                    news_item['score'],
                    news_item['comments']
                ]
                
                # ä¿å­˜åˆ°CSV
                df.to_csv(self.csv_file, index=False, encoding='utf-8')
                logging.debug(f"æ›´æ–°ç°æœ‰æ–°é—»åˆ†æ•°/è¯„è®º: {news_item['title']}")
                return False  # è¿”å›Falseè¡¨ç¤ºä¸æ˜¯æ–°å¢è®°å½•
            else:
                # æ·»åŠ æ–°è®°å½•
                new_row = {
                    'id': news_item['id'],  # ä¿æŒåŸå§‹ç±»å‹ï¼ŒCSVä¼šè‡ªåŠ¨è½¬æ¢
                    'title': news_item['title'],
                    'title_cn': news_item.get('title_cn', ''),
                    'url': news_item['url'],
                    'hn_url': news_item['hn_url'],
                    'score': news_item['score'],
                    'comments': news_item['comments'],
                    'content_summary': news_item.get('content_summary', ''),
                    'content_summary_cn': news_item.get('content_summary_cn', ''),
                    'crawl_time': datetime.now().isoformat(),
                    'sent_time': '',
                    'is_sent': False
                }
                df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
                
                # ä¿å­˜åˆ°CSV
                df.to_csv(self.csv_file, index=False, encoding='utf-8')
                logging.info(f"ä¿å­˜æ–°æ–°é—»: {news_item['title']}")
                return True  # è¿”å›Trueè¡¨ç¤ºæ˜¯æ–°å¢è®°å½•
            
        except Exception as e:
            logging.error(f"ä¿å­˜æ–°é—»å¤±è´¥: {e}")
            return False
    
    def mark_news_as_sent(self, news_id):
        """æ ‡è®°æ–°é—»ä¸ºå·²å‘é€"""
        try:
            df = self.load_news_data()
            
            # ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜ï¼šç»Ÿä¸€è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒ
            news_id_str = str(news_id)
            mask = (df['id'].astype(str) == news_id_str) & (df['is_sent'] == False)
            
            if mask.any():
                df.loc[mask, 'is_sent'] = True
                df.loc[mask, 'sent_time'] = datetime.now().isoformat()
                
                # ä¿å­˜åˆ°CSV
                df.to_csv(self.csv_file, index=False, encoding='utf-8')
                logging.debug(f"æ ‡è®°ä¸ºå·²å‘é€: {news_id}")
                return True
            else:
                logging.warning(f"æœªæ‰¾åˆ°å¯æ ‡è®°çš„æ–°é—»: {news_id}")
                return False
                
        except Exception as e:
            logging.error(f"æ ‡è®°å¤±è´¥: {e}")
            return False
    
    def get_unsent_news_from_csv(self):
        """è·å–æ‰€æœ‰æœªå‘é€çš„æ–°é—»ï¼ŒæŒ‰çˆ¬å–æ—¶é—´é¡ºåºï¼Œä¸é™åˆ¶æ•°é‡"""
        try:
            df = self.load_news_data()
            
            if df.empty:
                return []
            
            # åªç­›é€‰æœªå‘é€çš„æ–°é—»ï¼Œç§»é™¤ç¿»è¯‘æ¡ä»¶é™åˆ¶
            df['is_sent'] = df['is_sent'].fillna(False)
            df['is_sent'] = df['is_sent'].astype(bool)
            
            unsent = df[df['is_sent'] == False].copy()
            
            if unsent.empty:
                logging.info("æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æœªå‘é€æ–°é—»")
                return []
            
            # æŒ‰çˆ¬å–æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            unsent['crawl_time'] = pd.to_datetime(unsent['crawl_time'])
            unsent = unsent.sort_values('crawl_time', ascending=False)
            
            # å‘é€æ‰€æœ‰æœªå‘é€çš„æ–°é—»ï¼Œä¸é™åˆ¶æ•°é‡
            
            logging.info(f"æ‰¾åˆ° {len(unsent)} æ¡æœªå‘é€æ–°é—»")
            
            news_list = []
            for _, row in unsent.iterrows():
                news_list.append({
                    'id': row['id'],
                    'title': row['title'],
                    'title_cn': row['title_cn'] if pd.notna(row['title_cn']) else row['title'],  # å¦‚æœæ²¡æœ‰ç¿»è¯‘å°±ç”¨åŸæ ‡é¢˜
                    'url': row['url'],
                    'hn_url': row['hn_url'],
                    'score': int(row['score']),
                    'comments': int(row['comments']),
                    'content_summary': row['content_summary'] if pd.notna(row['content_summary']) else '',
                    'content_summary_cn': row['content_summary_cn'] if pd.notna(row['content_summary_cn']) else 'æš‚æ— å†…å®¹æ‘˜è¦',
                    'crawl_time': row['crawl_time']
                })
            
            return news_list
            
        except Exception as e:
            logging.error(f"è·å–æœªå‘é€æ–°é—»å¤±è´¥: {e}")
            return []
    
    def get_hn_frontpage(self):
        """è·å–HNé¦–é¡µæ‰€æœ‰æ–°é—»"""
        try:
            logging.info("ğŸ” å¼€å§‹è·å–HNé¦–é¡µæ–°é—»...")
            response = requests.get(
                self.base_url,
                headers=self.headers,
                proxies=self.proxies,
                timeout=self.request_timeout
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            news_items = []
            
            rows = soup.find_all('tr', class_='athing')
            
            # è·å–é¦–é¡µæ‰€æœ‰æ–°é—»ï¼Œä¸é™åˆ¶æ•°é‡
            for i, row in enumerate(rows):
                try:
                    news_id = row.get('id')
                    if not news_id:
                        continue
                    
                    title_cell = row.find('span', class_='titleline')
                    if not title_cell:
                        continue
                    
                    title_link = title_cell.find('a')
                    if not title_link:
                        continue
                    
                    title = title_link.get_text().strip()
                    url = title_link.get('href', '')
                    
                    if url.startswith('item?'):
                        url = urljoin(self.base_url, url)
                    
                    # è·å–åˆ†æ•°å’Œè¯„è®ºæ•°
                    next_row = row.find_next_sibling('tr')
                    score = 0
                    comments = 0
                    
                    if next_row:
                        score_span = next_row.find('span', class_='score')
                        if score_span:
                            score_text = score_span.get_text()
                            score = int(score_text.split()[0]) if score_text else 0
                        
                        comments_link = next_row.find('a', string=lambda text: text and 'comment' in text)
                        if comments_link:
                            comments_text = comments_link.get_text()
                            comments = int(comments_text.split()[0]) if comments_text.split()[0].isdigit() else 0
                    
                    # æ·»åŠ æ‰€æœ‰æ–°é—»ï¼Œä¸è¿‡æ»¤åˆ†æ•°
                    news_items.append({
                        'id': news_id,
                        'title': title,
                        'url': url,
                        'score': score,
                        'comments': comments,
                        'hn_url': f"{self.base_url}/item?id={news_id}",
                        'rank': i + 1  # ä¿å­˜åŸå§‹æ’å
                    })
                    
                except Exception as e:
                    logging.error(f"è§£ææ–°é—»å¤±è´¥: {e}")
                    continue
            
            logging.info(f"è·å–åˆ° {len(news_items)} æ¡æ–°é—»")
            return news_items
            
        except Exception as e:
            logging.error(f"è·å–HNé¦–é¡µå¤±è´¥: {e}")
            return []
    
    def get_article_content(self, url):
        """è·å–æ–‡ç« å†…å®¹ï¼Œæ”¹è¿›é”™è¯¯å¤„ç†"""
        try:
            if 'news.ycombinator.com' in url and '/item?' in url:
                return "è¿™æ˜¯ä¸€ä¸ªHNè®¨è®ºå¸–"
            
            # æ·»åŠ æ›´å¤šè¯·æ±‚å¤´ï¼Œé¿å…403é”™è¯¯
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            response = requests.get(
                url,
                headers=self.headers,
                proxies=self.proxies,
                timeout=self.request_timeout,
                allow_redirects=True
            )
            
            # å¦‚æœçŠ¶æ€ç ä¸æ˜¯200ï¼Œè¿”å›ç®€å•æè¿°
            if response.status_code != 200:
                logging.warning(f"HTTP {response.status_code} for {url}")
                return "æ— æ³•è·å–æ–‡ç« å†…å®¹"
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()
            
            # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹
            content = ""
            for selector in ['article', 'main', '.content', '.post', '.entry']:
                element = soup.select_one(selector)
                if element:
                    content = element.get_text()
                    break
            
            if not content:
                content = soup.get_text()
            
            # æ¸…ç†å†…å®¹
            lines = content.split('\n')
            cleaned_lines = []
            for line in lines:
                line = line.strip()
                if len(line) > 20 and not line.startswith(('http', 'www', '@')):
                    cleaned_lines.append(line)
            
            return '\n'.join(cleaned_lines[:8])  # å–å‰8è¡Œ
            
        except requests.exceptions.RequestException as e:
            logging.warning(f"ç½‘ç»œè¯·æ±‚å¤±è´¥ {url}: {e}")
            return "ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼Œæ— æ³•è·å–å†…å®¹"
        except Exception as e:
            logging.error(f"è·å–æ–‡ç« å†…å®¹å¤±è´¥ {url}: {e}")
            return "å†…å®¹è·å–å¤±è´¥"
    
    def translate_text(self, text):
        """æ”¹è¿›çš„ç¿»è¯‘åŠŸèƒ½"""
        if not text or len(text.strip()) < 3:
            return text
        
        try:
            import re
            
            # æ¸…ç†æ–‡æœ¬
            text = text.strip()
            text = re.sub(r'\s+', ' ', text)
            
            # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…APIé™åˆ¶
            if len(text) > self.max_translation_length:
                text = text[:self.max_translation_length] + "..."
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯ä¸­æ–‡
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
            if chinese_chars > len(text) * 0.3:
                return text
            
            # æ„å»ºç¿»è¯‘URL
            encoded_text = urllib.parse.quote(text)
            translate_url = f"https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=zh&dt=t&q={encoded_text}"
            
            response = requests.get(
                translate_url,
                headers=self.headers,
                proxies=self.proxies,
                timeout=self.translation_timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                if result and len(result) > 0 and len(result[0]) > 0:
                    translated = ""
                    for item in result[0]:
                        if item[0]:
                            translated += item[0]
                    
                    # æ¸…ç†ç¿»è¯‘ç»“æœ
                    translated = translated.strip()
                    translated = re.sub(r'\s+', ' ', translated)
                    
                    # ç®€å•çš„ç¿»è¯‘è´¨é‡æ£€æŸ¥
                    if len(translated) > 5 and translated != text:
                        return translated
            
            return text
            
        except Exception as e:
            logging.warning(f"ç¿»è¯‘å¤±è´¥: {e}")
            return text
    
    def clean_and_summarize_content(self, content):
        """å†…å®¹æ¸…ç†å’Œæ‘˜è¦"""
        if not content or len(content) < 30:
            return "æš‚æ— å†…å®¹æ‘˜è¦"
        
        import re
        
        # ç§»é™¤HTMLæ ‡ç­¾
        content = re.sub(r'<[^>]+>', '', content)
        
        # åˆ†å¥
        sentences = re.split(r'[.!?]+\s+', content)
        good_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if (20 <= len(sentence) <= 120 and
                not sentence.startswith(('http', 'www', '@', '#')) and
                sentence.count(' ') >= 2):
                good_sentences.append(sentence)
        
        # å–å‰2å¥
        summary = '. '.join(good_sentences[:2])
        if summary and not summary.endswith('.'):
            summary += '.'
        
        return summary if summary else "æš‚æ— å†…å®¹æ‘˜è¦"
    
    async def send_telegram_message(self, message, max_retries=None):
        """å‘é€Telegramæ¶ˆæ¯"""
        if max_retries is None:
            max_retries = self.message_max_retries
            
        for attempt in range(max_retries + 1):
            try:
                async with httpx.AsyncClient(
                    proxies=self.proxies,
                    timeout=self.telegram_timeout
                ) as client:
                    url = f"https://api.telegram.org/bot{self.bot_token}/sendMessage"
                    data = {
                        'chat_id': self.chat_id,
                        'text': message,
                        'parse_mode': 'HTML',
                        'disable_web_page_preview': False
                    }
                    
                    response = await client.post(url, data=data)
                    response.raise_for_status()
                    
                    await asyncio.sleep(self.message_send_interval)
                    return True
                    
            except httpx.TimeoutException:
                logging.warning(f"â° å‘é€è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries + 1})")
                if attempt < max_retries:
                    await asyncio.sleep(self.message_retry_interval)  # è¶…æ—¶åç­‰å¾…æ›´é•¿æ—¶é—´
                    
            except Exception as e:
                logging.error(f"âŒ å‘é€å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries + 1}): {e}")
                if attempt < max_retries:
                    await asyncio.sleep(self.message_send_interval)
                    
        return False
    
    def format_message(self, news, index, total):
        """å•†åŠ¡é£æ ¼çš„æ¶ˆæ¯æ ¼å¼ï¼Œç§»é™¤ç¿»è¯‘æ¡ä»¶é™åˆ¶"""
        # å¦‚æœæœ‰ä¸­æ–‡ç¿»è¯‘å°±ç”¨ç¿»è¯‘ï¼Œå¦åˆ™ç”¨åŸæ ‡é¢˜
        title = news.get('title_cn') if news.get('title_cn') and news.get('title_cn') != news['title'] else news['title']
        
        # æ‘˜è¦å¤„ç†ï¼šä¼˜å…ˆä½¿ç”¨ä¸­æ–‡æ‘˜è¦ï¼Œå…¶æ¬¡è‹±æ–‡æ‘˜è¦ï¼Œæœ€åé»˜è®¤æ–‡æœ¬
        summary = news.get('content_summary_cn')
        if not summary or summary == 'æš‚æ— å†…å®¹æ‘˜è¦':
            summary = news.get('content_summary', 'æš‚æ— å†…å®¹æ‘˜è¦')
        if not summary:
            summary = "æš‚æ— å†…å®¹æ‘˜è¦"
        
        # é™åˆ¶æ‘˜è¦é•¿åº¦
        if len(summary) > 180:
            summary = summary[:180] + "..."
        
        # å•†åŠ¡åŒ–çš„åˆ†æ•°ç­‰çº§æè¿°
        if news['score'] > 500:
            popularity = "ğŸ”¥ çƒ­é—¨è¯é¢˜"
            score_level = "æé«˜å…³æ³¨"
        elif news['score'] > 200:
            popularity = "â­ é«˜åº¦å…³æ³¨"
            score_level = "é«˜å…³æ³¨åº¦"
        elif news['score'] > 100:
            popularity = "ğŸ“ˆ æŒç»­å…³æ³¨"
            score_level = "ä¸­ç­‰å…³æ³¨"
        else:
            popularity = "ğŸ“Š æ–°å…´è¯é¢˜"
            score_level = "åˆæœŸå…³æ³¨"
        
        # è®¨è®ºæ´»è·ƒåº¦æè¿°
        if news['comments'] > 100:
            discussion = "ğŸ’¬ è®¨è®ºçƒ­çƒˆ"
        elif news['comments'] > 50:
            discussion = "ğŸ’­ è®¨è®ºæ´»è·ƒ"
        elif news['comments'] > 10:
            discussion = "ğŸ“ æœ‰æ‰€è®¨è®º"
        else:
            discussion = "ğŸ” å¾…æ·±å…¥è®¨è®º"
        
        # æ—¶é—´æ ¼å¼åŒ–
        crawl_time = ""
        if 'crawl_time' in news:
            try:
                if isinstance(news['crawl_time'], str):
                    dt = datetime.fromisoformat(news['crawl_time'].replace('Z', '+00:00'))
                else:
                    dt = news['crawl_time']
                crawl_time = dt.strftime("%H:%M")
            except:
                crawl_time = ""
        
        # å•†åŠ¡é£æ ¼çš„æ¶ˆæ¯æ ¼å¼
        message = f"""<b>ğŸ“° Hacker News ç§‘æŠ€èµ„è®¯ #{index}</b>
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

<b>ğŸ”¥ {title}</b>

<b>ğŸ“Š æ•°æ®æ¦‚è§ˆ</b>
â€¢ {popularity} ({news['score']} åˆ†)
â€¢ {discussion} ({news['comments']} æ¡è¯„è®º)
â€¢ å‘å¸ƒæ—¶é—´: {crawl_time}

<b>ğŸ“ å†…å®¹æ‘˜è¦</b>
{summary}

<b>ğŸ”— ç›¸å…³é“¾æ¥</b>
â€¢ <a href="{news['url']}">æŸ¥çœ‹åŸæ–‡</a>
â€¢ <a href="{news['hn_url']}">å‚ä¸è®¨è®º</a>

<i>ç¬¬ {index} æ¡ï¼Œå…± {total} æ¡èµ„è®¯</i>"""
        
        return message
    
    async def send_batch_header(self, count):
        """ç®€åŒ–çš„æ‰¹æ¬¡å¼€å§‹æ¶ˆæ¯"""
        current_time = datetime.now().strftime("%H:%M")
        
        header = f"""ğŸ“° <b>HNç§‘æŠ€èµ„è®¯</b> ({count}æ¡)

â° {current_time} | ğŸ”¥ news.ycombinator.com

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"""
        
        try:
            await self.bot.send_message(
                chat_id=self.chat_id,
                text=header,
                parse_mode='HTML'
            )
        except Exception as e:
            logging.error(f"å‘é€æ‰¹æ¬¡æ¶ˆæ¯å¤±è´¥: {e}")
    
    async def send_completion_message(self, success_count, total_count):
        """ç®€åŒ–çš„å®Œæˆæ¶ˆæ¯"""
        next_time = (datetime.now() + timedelta(minutes=int(os.getenv('CHECK_INTERVAL_MINUTES', 1)))).strftime('%H:%M')
        
        completion_msg = f"""âœ… <b>æ¨é€å®Œæˆ</b>

ğŸ“Š æˆåŠŸ: {success_count}/{total_count}
â° ä¸‹æ¬¡: {next_time}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"""
        
        try:
            await self.bot.send_message(
                chat_id=self.chat_id,
                text=completion_msg,
                parse_mode='HTML'
            )
        except Exception as e:
            logging.error(f"å‘é€å®Œæˆæ¶ˆæ¯å¤±è´¥: {e}")
    
    async def crawl_and_send(self):
        """ä¸»è¦çˆ¬å–å’Œå‘é€é€»è¾‘ï¼Œä¼˜åŒ–å»é‡"""
        logging.info("å¼€å§‹çˆ¬å–...")
        
        # è·å–æ–°é—»
        news_list = self.get_hn_frontpage()
        if not news_list:
            logging.warning("æœªè·å–åˆ°æ–°é—»")
            return
        
        # åŠ è½½ç°æœ‰æ•°æ®ï¼Œé¿å…é‡å¤å¤„ç†
        existing_df = self.load_news_data()
        # ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜ï¼šå°†IDç»Ÿä¸€è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒ
        existing_ids = set(str(id_val) for id_val in existing_df['id'].tolist()) if not existing_df.empty else set()
        
        # å¤„ç†æ–°é—»
        new_count = 0
        updated_count = 0
        processed_count = 0
        
        for news in news_list:
            try:
                processed_count += 1
                # ç¡®ä¿æ–°é—»IDä¹Ÿæ˜¯å­—ç¬¦ä¸²
                news_id = str(news['id'])
                
                # å¦‚æœæ–°é—»å·²å­˜åœ¨ï¼Œåªæ›´æ–°åˆ†æ•°å’Œè¯„è®ºæ•°
                if news_id in existing_ids:
                    # é€šè¿‡save_news_to_csvæ¥æ›´æ–°ï¼Œå®ƒä¼šæ­£ç¡®å¤„ç†ç°æœ‰è®°å½•
                    self.save_news_to_csv(news)
                    updated_count += 1
                    logging.info(f"æ›´æ–°ç°æœ‰æ–°é—» ({processed_count}/{len(news_list)}): {news['title']}")
                    continue
                
                # å¤„ç†æ–°æ–°é—»
                logging.info(f"å¤„ç†æ–°æ–°é—» ({processed_count}/{len(news_list)}): {news['title']}")
                
                # è·å–å†…å®¹
                content = self.get_article_content(news['url'])
                
                # ç¿»è¯‘æ ‡é¢˜
                news['title_cn'] = self.translate_text(news['title'])
                
                # å¤„ç†æ‘˜è¦
                news['content_summary'] = self.clean_and_summarize_content(content)
                news['content_summary_cn'] = self.translate_text(news['content_summary'])
                
                # ä¿å­˜åˆ°CSV
                if self.save_news_to_csv(news):
                    new_count += 1
                    existing_ids.add(news_id)  # æ·»åŠ åˆ°å·²å­˜åœ¨IDé›†åˆ
                
                await asyncio.sleep(0.3)  # é¿å…è¯·æ±‚è¿‡å¿«
                
            except Exception as e:
                logging.error(f"å¤„ç†æ–°é—»å¤±è´¥: {e}")
                continue
        
        # åˆ†æ•°å’Œè¯„è®ºæ•°çš„æ›´æ–°å·²ç»åœ¨save_news_to_csvä¸­å¤„ç†äº†
        if updated_count > 0:
            logging.info(f"æ›´æ–° {updated_count} æ¡ç°æœ‰æ–°é—»çš„åˆ†æ•°/è¯„è®ºæ•°")
        
        if new_count > 0:
            logging.info(f"æ–°å¢ {new_count} æ¡æ–°é—»")
        else:
            logging.info("æ²¡æœ‰æ–°å¢æ–°é—»")
        
        # è·å–æœªå‘é€çš„æ–°é—»
        unsent_news = self.get_unsent_news_from_csv()
        
        if not unsent_news:
            logging.info("æ²¡æœ‰æ–°é—»éœ€è¦å‘é€")
            return
        
        logging.info(f"å‡†å¤‡å‘é€ {len(unsent_news)} æ¡æ–°é—»")
        
        # å‘é€æ–°é—»
        success_count = 0
        for i, news in enumerate(unsent_news, 1):
            try:
                message = self.format_message(news, i, len(unsent_news))
                success = await self.send_telegram_message(message)
                
                if success:
                    self.mark_news_as_sent(news['id'])
                    success_count += 1
                    logging.info(f"âœ… å‘é€æˆåŠŸ ({i}/{len(unsent_news)}): {news['title']}")
                else:
                    logging.error(f"âŒ å‘é€å¤±è´¥ ({i}/{len(unsent_news)}): {news['title']}")
                
                # æ§åˆ¶å‘é€é¢‘ç‡ï¼Œé¿å…APIé™åˆ¶
                if i < len(unsent_news):  # ä¸æ˜¯æœ€åä¸€æ¡æ¶ˆæ¯
                    await asyncio.sleep(self.request_interval)  # é¿å…è¯·æ±‚è¿‡å¿«
                
            except Exception as e:
                logging.error(f"âŒ å¤„ç†æ–°é—»å¤±è´¥: {e}")
        
        # å‘é€å®Œæˆæ¶ˆæ¯
        if len(unsent_news) > 0:
            # å¤§æ‰¹é‡å‘é€æ—¶å¢åŠ å»¶è¿Ÿ
            delay = self.bulk_message_interval if len(unsent_news) > 20 else self.message_send_interval
            await asyncio.sleep(delay)
        
        if success_count > 0:
            try:
                await self.send_completion_message(success_count, len(unsent_news))
            except Exception as e:
                logging.warning(f"å‘é€å®Œæˆæ¶ˆæ¯å¤±è´¥: {e}")
        
        logging.info(f"å‘é€å®Œæˆ: {success_count}/{len(unsent_news)}")

    def test_network_connection(self):
        """æµ‹è¯•ç½‘ç»œè¿æ¥"""
        try:
            # æµ‹è¯•HNç½‘ç«™è¿æ¥
            response = requests.get(
                self.base_url,
                headers=self.headers,
                proxies=self.proxies,
                timeout=self.connection_test_timeout
            )
            
            if response.status_code == 200:
                logging.info("âœ… HNç½‘ç«™è¿æ¥æ­£å¸¸")
            else:
                logging.warning(f"âš ï¸ HNç½‘ç«™è¿æ¥å¼‚å¸¸: HTTP {response.status_code}")
            
            # æµ‹è¯•Telegram APIè¿æ¥
            if self.proxies:
                import httpx
                proxy_url = self.proxies.get('https', self.proxies.get('http'))
                
                async def test_telegram():
                    try:
                        async with httpx.AsyncClient(proxies=proxy_url, timeout=self.connection_test_timeout) as client:
                            response = await client.get(f"https://api.telegram.org/bot{self.bot_token}/getMe")
                            if response.status_code == 200:
                                result = response.json()
                                if result.get('ok'):
                                    logging.info("âœ… Telegram API è¿æ¥æ­£å¸¸")
                                    return True
                        return False
                    except Exception as e:
                        logging.warning(f"âš ï¸ Telegram API è¿æ¥å¤±è´¥: {e}")
                        return False
                
                # è¿è¡Œå¼‚æ­¥æµ‹è¯•
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        # å¦‚æœå·²ç»åœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼Œåˆ›å»ºæ–°çš„ä»»åŠ¡
                        import concurrent.futures
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            future = executor.submit(asyncio.run, test_telegram())
                            return future.result()
                    else:
                        return asyncio.run(test_telegram())
                except Exception as e:
                    logging.error(f"âŒ å¼‚æ­¥æµ‹è¯•å¤±è´¥: {e}")
                    return False
            else:
                # ç›´è¿æ¨¡å¼æµ‹è¯• - ä½¿ç”¨åŒæ­¥æ–¹å¼
                try:
                    # ç®€å•æµ‹è¯•Telegram APIï¼ˆä½¿ç”¨requestsï¼‰
                    telegram_response = requests.get(
                        f"https://api.telegram.org/bot{self.bot_token}/getMe",
                        proxies=self.proxies,
                        timeout=self.connection_test_timeout
                    )
                    
                    if telegram_response.status_code == 200:
                        result = telegram_response.json()
                        if result.get('ok'):
                            bot_info = result.get('result', {})
                            logging.info(f"âœ… Telegram APIè¿æ¥æ­£å¸¸ - Bot: {bot_info.get('username', 'Unknown')}")
                            return True
                    
                    logging.warning(f"âš ï¸ Telegram APIè¿æ¥å¼‚å¸¸: {telegram_response.status_code}")
                    return False
                except Exception as e:
                    logging.error(f"âŒ Telegram APIè¿æ¥å¤±è´¥: {e}")
                    return False
                    
        except Exception as e:
            logging.error(f"âŒ ç½‘ç»œè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    enable_web = os.getenv('ENABLE_WEB', 'false').lower() == 'true'
    
    if enable_web:
        logging.info("ğŸŒ WebåŠŸèƒ½å·²å¯ç”¨")
        # WebåŠŸèƒ½ä»£ç ...
    else:
        logging.info("ğŸš« WebåŠŸèƒ½å·²ç¦ç”¨")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹å¹¶æµ‹è¯•è¿æ¥
    crawler = HackerNewsCrawler()
    
    # æµ‹è¯•ç½‘ç»œè¿æ¥
    if not crawler.test_network_connection():
        logging.error("âŒ ç½‘ç»œè¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç†é…ç½®")
        return
    
    # å®šä¹‰è¿è¡Œå‡½æ•°ï¼Œä½¿ç”¨åŒä¸€ä¸ªçˆ¬è™«å®ä¾‹
    def run_crawler_instance():
        """è¿è¡Œçˆ¬è™«å®ä¾‹"""
        try:
            asyncio.run(crawler.crawl_and_send())
        except Exception as e:
            logging.error(f"çˆ¬è™«æ‰§è¡Œå¤±è´¥: {e}")
    
    # ç«‹å³æ‰§è¡Œä¸€æ¬¡
    logging.info("ğŸš€ ç«‹å³æ‰§è¡Œç¬¬ä¸€æ¬¡çˆ¬å–...")
    run_crawler_instance()
    
    # è®¾ç½®å®šæ—¶ä»»åŠ¡
    interval_minutes = int(os.getenv('CHECK_INTERVAL_MINUTES', 5))
    schedule.every(interval_minutes).minutes.do(run_crawler_instance)
    logging.info(f"â° å®šæ—¶ä»»åŠ¡: æ¯ {interval_minutes} åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡")
    
    # è¿è¡Œå®šæ—¶ä»»åŠ¡
    daemon_check_interval = int(os.getenv('DAEMON_CHECK_INTERVAL', 30))
    try:
        while True:
            schedule.run_pending()
            time.sleep(daemon_check_interval)
    except KeyboardInterrupt:
        logging.info("ğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logging.error(f"âŒ å®šæ—¶ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main() 
